{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\yomancool\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\yomancool\\anaconda3\\lib\\site-packages (from textblob)\n",
      "Requirement already satisfied: six in c:\\users\\yomancool\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "import pip\n",
    "\n",
    "def install(package):\n",
    "   pip.main(['install', package])\n",
    "\n",
    "install('textblob') \n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_review_helpful(g_id, u_id):\n",
    "    list_of_review = []\n",
    "    for r_id in review:\n",
    "        if review[r_id]['game_id'] == g_id:\n",
    "            list_of_review.append(r_id)\n",
    "    sl = []\n",
    "    target = []\n",
    "    for i in range(len(list_of_review)):\n",
    "        r_id = list_of_review[i]\n",
    "        score = review[r_id]['helpful']\n",
    "        sl.append(score)\n",
    "        if u_id == review[r_id]['user_id']:\n",
    "            target.append(i)\n",
    "    #print('total review = ', len(list_of_review))\n",
    "    #print('target len = ', len(target))\n",
    "    if len(target) == 0:\n",
    "        return 0\n",
    "    if np.std(sl) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        z = zscore(sl)\n",
    "        val = [z[x] for x in target]\n",
    "        return np.mean(val)\n",
    "\n",
    "def get_review_normalized(g_id, u_id):\n",
    "    list_of_review = u_review[u_id]\n",
    "    p = []\n",
    "    s = []\n",
    "    target = []\n",
    "    if len(list_of_review) <= 1:\n",
    "        return (0,0)\n",
    "    for i in range(len(list_of_review)):\n",
    "        r_id = list_of_review[i]\n",
    "        tup = get_review_pos_score(r_id)\n",
    "        p.append(tup[0])\n",
    "        s.append(tup[1])\n",
    "        if g_id == review[r_id]['game_id']:\n",
    "            target.append(i)\n",
    "    #print('total review = ', len(list_of_review))\n",
    "    #print('target len = ', len(target))\n",
    "    if len(target) == 0:\n",
    "        return (0,0)\n",
    "    if np.std(p) == 0:\n",
    "        a = 0\n",
    "    else:\n",
    "        #print(p)\n",
    "        zp = zscore(p)\n",
    "        val_p = [zp[x] for x in target]\n",
    "        a = np.mean(val_p)\n",
    "    \n",
    "    if np.std(s) == 0:\n",
    "        b = 0\n",
    "    else:\n",
    "        #print(s)\n",
    "        zs = zscore(s)\n",
    "        val_s = [zs[x] for x in target]\n",
    "        b = np.mean(val_s)\n",
    "    return (a, b)\n",
    "\n",
    "def get_review_pos_score(r_id):\n",
    "    pattern = TextBlob(review[r_id]['content'])\n",
    "    pol = pattern.sentiment[0]\n",
    "    sub = pattern.sentiment[1]\n",
    "    return (pol, sub)\n",
    "\n",
    "def get_num_games(g_id, u_id):\n",
    "    target = \"\"\n",
    "    for g in classification:\n",
    "        if g_id in classification[g]:\n",
    "            target = g\n",
    "            break\n",
    "    return user[u_id][target]\n",
    "\n",
    "def get_avg_user_level(g_id):\n",
    "    level_total = 0\n",
    "    cnt = 0\n",
    "    for u in game_user[g_id]:\n",
    "        if u in user:\n",
    "            level_total = level_total + user[u]['userLevel']\n",
    "            cnt = cnt + 1\n",
    "    return level_total/cnt\n",
    "\n",
    "def get_avg_user_playtime(g_id):\n",
    "    playtime_total = 0\n",
    "    cnt = 0\n",
    "    for u in game_user[g_id]:\n",
    "        playtime_total = playtime_total + game_user[g_id][u]['total_play_time']\n",
    "        cnt = cnt + 1\n",
    "    return playtime_total/cnt\n",
    "\n",
    "def get_avg_user_playtime_in_G(g_id):\n",
    "    playtime_total = 0\n",
    "    num_user = 0\n",
    "    target = \"\"\n",
    "    for g in classification:\n",
    "        if g_id in classification[g]:\n",
    "            target = g\n",
    "            break\n",
    "    for all_other_g in classification[target]:\n",
    "        if all_other_g not in result:\n",
    "            continue\n",
    "        for u in game_user[all_other_g]:\n",
    "            playtime_total = playtime_total + game_user[all_other_g][u]['total_play_time']\n",
    "            num_user = num_user + len(game_user[all_other_g])\n",
    "    return playtime_total/num_user\n",
    "\n",
    "def get_user_playtime(g_id, u_id):\n",
    "    return game_user[g_id][u_id]['total_play_time']\n",
    "\n",
    "def get_user_level(u_id):\n",
    "    return user[u_id]['userLevel']\n",
    "\n",
    "#remember to load pickle file: review.p\n",
    "def get_total_review_g(g_id):\n",
    "    count = 0\n",
    "    for each in review:\n",
    "        if review[each]['game_id'] == g_id:\n",
    "            count = count + 1\n",
    "    #print('{0} : {1} '.format(g_id,count))\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_total_review_u(u_id):\n",
    "    count = 0\n",
    "    for each in review:\n",
    "        if review[each]['user_id'] == u_id:\n",
    "            count = count + 1\n",
    "    #print('{0} : {1} '.format(u_id,count))\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28413\n",
      "1000 28413\n",
      "2000 28413\n",
      "3000 28413\n",
      "4000 28413\n",
      "5000 28413\n",
      "6000 28413\n",
      "7000 28413\n",
      "8000 28413\n",
      "9000 28413\n",
      "10000 28413\n",
      "11000 28413\n",
      "12000 28413\n",
      "13000 28413\n",
      "14000 28413\n",
      "14207\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "with open(\"dicts/result.p\", \"rb\") as f:\n",
    "    result = pickle.load(f)\n",
    "with open(\"dicts/classification.p\", \"rb\") as f:\n",
    "    classification = pickle.load(f)\n",
    "with open(\"dicts/userDictionary_sports.p\", \"rb\") as f:\n",
    "    user = pickle.load(f)\n",
    "with open(\"dicts/game_user.p\", \"rb\") as f:\n",
    "    game_user = pickle.load(f)\n",
    "with open(\"dicts/review.p\", \"rb\") as f:\n",
    "    review = pickle.load(f)\n",
    "with open(\"dicts/user_review.p\", \"rb\") as f:\n",
    "    u_review = pickle.load(f)\n",
    "            \n",
    "cnt = 0\n",
    "#print(result['4290'])\n",
    "for key in result:\n",
    "    if key not in game_user:\n",
    "        result.pop(key)\n",
    "\n",
    "for i in result:\n",
    "    cnt = cnt + len(result[i])\n",
    "#print(result)\n",
    "test_size = cnt * 0.2\n",
    "test = {}\n",
    "train = {}\n",
    "while len(test) < test_size:\n",
    "    g_id, val = random.choice(list(result.items()))\n",
    "    u_id, res = random.choice(list(val.items()))\n",
    "    tup = (g_id, u_id)\n",
    "    if tup not in test:\n",
    "        test[tup] = res\n",
    "\n",
    "for g_id in result:\n",
    "    for u_id in result[g_id]:\n",
    "        tup = (g_id, u_id)\n",
    "        #print(tup)\n",
    "        if tup not in test:\n",
    "            #print(result[g_id][u_id])\n",
    "            train[tup] = result[g_id][u_id]\n",
    "            \n",
    "#print(list(game_user.keys()))\n",
    "len_train = len(train)\n",
    "N = round(len_train/2)\n",
    "#N = 10\n",
    "X_train = np.zeros((N+1, 11))\n",
    "Y_train = np.zeros((N+1, 1))\n",
    "cnt = 0\n",
    "for key in train:\n",
    "    #print(key, train[key])\n",
    "    g_id = key[0]\n",
    "    u_id = key[1]\n",
    "    #l = []\n",
    "    X_train[cnt,0] = get_num_games(g_id, u_id)\n",
    "    X_train[cnt,1] = get_avg_user_level(g_id)\n",
    "    X_train[cnt,2] = get_avg_user_playtime(g_id)\n",
    "    X_train[cnt,3] = get_avg_user_playtime_in_G(g_id)\n",
    "    X_train[cnt,4] = get_user_playtime(g_id, u_id)\n",
    "    X_train[cnt,5] = get_user_level(u_id)\n",
    "    X_train[cnt,6] = get_total_review_g(g_id)\n",
    "    X_train[cnt,7] = get_total_review_u(u_id)\n",
    "    X_train[cnt,8], X_train[cnt,9] = get_review_normalized(g_id, u_id)\n",
    "    X_train[cnt,10] = get_review_helpful(g_id, u_id)\n",
    "    Y_train[cnt] = result[g_id][u_id]\n",
    "    if cnt % 1000 == 0:\n",
    "        print(cnt, len_train)\n",
    "    cnt = cnt + 1\n",
    "    if cnt > N:\n",
    "        break\n",
    "print(cnt)\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28413\n",
      "100 28413\n",
      "200 28413\n",
      "300 28413\n",
      "400 28413\n",
      "500 28413\n",
      "600 28413\n",
      "700 28413\n",
      "800 28413\n",
      "900 28413\n",
      "1000 28413\n",
      "1100 28413\n",
      "1200 28413\n",
      "1300 28413\n",
      "1400 28413\n",
      "1500 28413\n",
      "1600 28413\n",
      "1700 28413\n",
      "1800 28413\n",
      "1900 28413\n",
      "2000 28413\n",
      "2100 28413\n",
      "2200 28413\n",
      "2300 28413\n",
      "2400 28413\n",
      "2500 28413\n",
      "2600 28413\n",
      "2700 28413\n",
      "2800 28413\n",
      "2842\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "#len_train = len(train)\n",
    "X_test = np.zeros((round(N/5)+1, 11))\n",
    "Y_test = np.zeros((round(N/5)+1, 1))\n",
    "cnt = 0\n",
    "for key in test:\n",
    "    #print(key, train[key])\n",
    "    g_id = key[0]\n",
    "    u_id = key[1]\n",
    "    #l = []\n",
    "    if u_id not in user:\n",
    "        continue\n",
    "    X_test[cnt,0] = get_num_games(g_id, u_id)\n",
    "    X_test[cnt,1] = get_avg_user_level(g_id)\n",
    "    X_test[cnt,2] = get_avg_user_playtime(g_id)\n",
    "    X_test[cnt,3] = get_avg_user_playtime_in_G(g_id)\n",
    "    X_test[cnt,4] = get_user_playtime(g_id, u_id)\n",
    "    X_test[cnt,5] = get_user_level(u_id)\n",
    "    X_test[cnt,6] = get_total_review_g(g_id)\n",
    "    X_test[cnt,7] = get_total_review_u(u_id)\n",
    "    X_test[cnt,8], X_test[cnt,9] = get_review_normalized(g_id, u_id)\n",
    "    X_test[cnt,10] = get_review_helpful(g_id, u_id)\n",
    "    Y_test[cnt] = int(result[g_id][u_id])\n",
    "    if cnt % 100 == 0:\n",
    "        print(cnt, len_train)\n",
    "    cnt = cnt + 1\n",
    "    if cnt > N/5:\n",
    "        break\n",
    "print(cnt)\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grid\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import fbeta_score, make_scorer, average_precision_score, classification_report\n",
    "\n",
    "parameter_candidates = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "# Create a classifier object with the classifier and parameter candidates\n",
    "print('start grid')\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=parameter_candidates, n_jobs=-1)\n",
    "clf.fit(X_train, np.ravel(Y_train))\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for data1: 0.78848454987\n",
      "Best C: 1\n",
      "Best Kernel: linear\n",
      "Best Gamma: auto\n"
     ]
    }
   ],
   "source": [
    "print('Best score for data1:', clf.best_score_)\n",
    "print('Best C:',clf.best_estimator_.C)\n",
    "print('Best Kernel:',clf.best_estimator_.kernel)\n",
    "print('Best Gamma:',clf.best_estimator_.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69598874032371572"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,np.ravel(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_candidates = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [10,1,0.1,0.01]}]\n",
    "\n",
    "# Create a classifier object with the classifier and parameter candidates\n",
    "print('start grid')\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=parameter_candidates, n_jobs=-1)\n",
    "clf.fit(X_train, np.ravel(Y_train))\n",
    "print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
