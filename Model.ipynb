{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_games(g_id, u_id):\n",
    "    target = \"\"\n",
    "    for g in classification:\n",
    "        if g_id in classification[g]:\n",
    "            target = g\n",
    "            break\n",
    "    return user[u_id][target]\n",
    "\n",
    "def get_avg_user_level(g_id):\n",
    "    level_total = 0\n",
    "    cnt = 0\n",
    "    for u in game_user[g_id]:\n",
    "        if u in user:\n",
    "            level_total = level_total + user[u]['userLevel']\n",
    "            cnt = cnt + 1\n",
    "    return level_total/cnt\n",
    "\n",
    "def get_avg_user_playtime(g_id):\n",
    "    playtime_total = 0\n",
    "    cnt = 0\n",
    "    for u in game_user[g_id]:\n",
    "        playtime_total = playtime_total + game_user[g_id][u]['total_play_time']\n",
    "        cnt = cnt + 1\n",
    "    return playtime_total/cnt\n",
    "\n",
    "def get_avg_user_playtime_in_G(g_id):\n",
    "    playtime_total = 0\n",
    "    num_user = 0\n",
    "    target = \"\"\n",
    "    for g in classification:\n",
    "        if g_id in classification[g]:\n",
    "            target = g\n",
    "            break\n",
    "    for all_other_g in classification[target]:\n",
    "        if all_other_g not in result:\n",
    "            continue\n",
    "        for u in game_user[all_other_g]:\n",
    "            playtime_total = playtime_total + game_user[all_other_g][u]['total_play_time']\n",
    "            num_user = num_user + len(game_user[all_other_g])\n",
    "    return playtime_total/num_user\n",
    "\n",
    "def get_user_playtime(g_id, u_id):\n",
    "    return game_user[g_id][u_id]['total_play_time']\n",
    "\n",
    "def get_user_level(u_id):\n",
    "    return user[u_id]['userLevel']\n",
    "\n",
    "#remember to load pickle file: review.p\n",
    "def get_total_review_g(g_id):\n",
    "    count = 0\n",
    "    for each in review:\n",
    "        if review[each]['game_id'] == g_id:\n",
    "            count = count + 1\n",
    "    #print('{0} : {1} '.format(g_id,count))\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_total_review_u(u_id):\n",
    "    count = 0\n",
    "    for each in review:\n",
    "        if review[each]['user_id'] == u_id:\n",
    "            count = count + 1\n",
    "    #print('{0} : {1} '.format(u_id,count))\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 60728\n",
      "1000 60728\n",
      "2000 60728\n",
      "3000 60728\n",
      "4000 60728\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "with open(\"dicts/result.p\", \"rb\") as f:\n",
    "    result = pickle.load(f)\n",
    "with open(\"dicts/classification.p\", \"rb\") as f:\n",
    "    classification = pickle.load(f)\n",
    "with open(\"dicts/userDictionary_racing.p\", \"rb\") as f:\n",
    "    user = pickle.load(f)\n",
    "with open(\"dicts/game_user.p\", \"rb\") as f:\n",
    "    game_user = pickle.load(f)\n",
    "with open(\"dicts/review.p\", \"rb\") as f:\n",
    "    review = pickle.load(f)\n",
    "\n",
    "            \n",
    "cnt = 0\n",
    "#print(result['4290'])\n",
    "for key in result:\n",
    "    if key not in game_user:\n",
    "        result.pop(key)\n",
    "\n",
    "for i in result:\n",
    "    cnt = cnt + len(result[i])\n",
    "#print(result)\n",
    "test_size = cnt * 0.2\n",
    "test = {}\n",
    "train = {}\n",
    "while len(test) < test_size:\n",
    "    g_id, val = random.choice(list(result.items()))\n",
    "    u_id, res = random.choice(list(val.items()))\n",
    "    tup = (g_id, u_id)\n",
    "    if tup not in test:\n",
    "        test[tup] = res\n",
    "#print(test)\n",
    "for g_id in result:\n",
    "    for u_id in result[g_id]:\n",
    "        tup = (g_id, u_id)\n",
    "        #print(tup)\n",
    "        if tup not in test:\n",
    "            #print(result[g_id][u_id])\n",
    "            train[tup] = result[g_id][u_id]\n",
    "            \n",
    "#print(list(game_user.keys()))\n",
    "len_train = len(train)\n",
    "N = 4000\n",
    "X_train = np.zeros((N+1, 8))\n",
    "Y_train = np.zeros((N+1, 1))\n",
    "cnt = 0\n",
    "for key in train:\n",
    "    #print(key, train[key])\n",
    "    g_id = key[0]\n",
    "    u_id = key[1]\n",
    "    #l = []\n",
    "    X_train[cnt,0] = get_num_games(g_id, u_id)\n",
    "    X_train[cnt,1] = get_avg_user_level(g_id)\n",
    "    X_train[cnt,2] = get_avg_user_playtime(g_id)\n",
    "    X_train[cnt,3] = get_avg_user_playtime_in_G(g_id)\n",
    "    X_train[cnt,4] = get_user_playtime(g_id, u_id)\n",
    "    X_train[cnt,5] = get_user_level(u_id)\n",
    "    X_train[cnt,6] = get_total_review_g(g_id)\n",
    "    X_train[cnt,7] = get_total_review_u(u_id)\n",
    "    Y_train[cnt] = result[g_id][u_id]\n",
    "    if cnt % 1000 == 0:\n",
    "        print(cnt, len_train)\n",
    "    cnt = cnt + 1\n",
    "    if cnt > N:\n",
    "        break\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:2: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 60728\n"
     ]
    }
   ],
   "source": [
    "#len_train = len(train)\n",
    "X_test = np.zeros(((N/5)+1, 8))\n",
    "Y_test = np.zeros(((N/5)+1, 1))\n",
    "cnt = 0\n",
    "for key in test:\n",
    "    #print(key, train[key])\n",
    "    g_id = key[0]\n",
    "    u_id = key[1]\n",
    "    #l = []\n",
    "    if u_id not in user:\n",
    "        continue\n",
    "    X_test[cnt,0] = get_num_games(g_id, u_id)\n",
    "    X_test[cnt,1] = get_avg_user_level(g_id)\n",
    "    X_test[cnt,2] = get_avg_user_playtime(g_id)\n",
    "    X_test[cnt,3] = get_avg_user_playtime_in_G(g_id)\n",
    "    X_test[cnt,4] = get_user_playtime(g_id, u_id)\n",
    "    X_test[cnt,5] = get_user_level(u_id)\n",
    "    X_test[cnt,6] = get_total_review_g(g_id)\n",
    "    X_test[cnt,7] = get_total_review_u(u_id)\n",
    "    Y_test[cnt] = int(result[g_id][u_id])\n",
    "    if cnt % 1000 == 0:\n",
    "        print(cnt, len_train)\n",
    "    cnt = cnt + 1\n",
    "    if cnt > N/5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65543071161\n",
      "0.660424469413\n"
     ]
    }
   ],
   "source": [
    "#N = 4000\n",
    "#X_small = X_train[1:N,:]\n",
    "#Y_small = Y_train[1:N,:]\n",
    "#X_st = X_test[1:N/5,:]\n",
    "#Y_st = Y_test[1:N/5,:]\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model_S = SVC(kernel=\"linear\", probability = True)\n",
    "model_R = SVC(kernel=\"rbf\", probability = True)\n",
    "model_S.fit(X_train, np.ravel(Y_train))\n",
    "model_R.fit(X_train,np.ravel(Y_train))\n",
    "print(model_S.score(X_test, np.ravel(Y_test)))\n",
    "print(model_R.score(X_test, np.ravel(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/Yomancool/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.440 (+/-0.133) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.438 (+/-0.111) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.606 (+/-0.438) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.467 (+/-0.122) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.608 (+/-0.432) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.505 (+/-0.187) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.606 (+/-0.434) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.445 (+/-0.138) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.488 (+/-0.137) for {'C': 1, 'kernel': 'linear'}\n",
      "0.489 (+/-0.138) for {'C': 10, 'kernel': 'linear'}\n",
      "0.493 (+/-0.140) for {'C': 100, 'kernel': 'linear'}\n",
      "0.488 (+/-0.137) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bc2b65f067c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mY_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import fbeta_score, make_scorer, average_precision_score, classification_report\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, scoring = '%s_macro' % score)\n",
    "    clf.fit(X_train, np.ravel(Y_train))\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    Y_true, Y_pred = Y_test, clf.predict(X_test)\n",
    "    print(classification_report(Y_true, Y_pred))\n",
    "    print()\n",
    "\n",
    "\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
